{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3f416dd-9f7d-4d5c-98f8-60d9c86c7c82",
   "metadata": {},
   "source": [
    "### Churn @ Robinhood\n",
    "#### Load Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db448b89-e3eb-4152-b27d-88017574821c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: CUDA Error: Error at driver init: Call to cuInit results in CUDA_ERROR_UNKNOWN (999)\n"
     ]
    }
   ],
   "source": [
    "from numba import cuda\n",
    "\n",
    "try:\n",
    "    print(\"Available GPUs:\", cuda.gpus)\n",
    "except cuda.CudaSupportError as e:\n",
    "    print(\"CUDA Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6188b919-7526-4909-ab1e-62af0eac877b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oem/Documents/github/magnimind_projects/magpenv/lib/python3.12/site-packages/cudf/utils/_ptxcompiler.py:64: UserWarning: Error getting driver and runtime versions:\n",
      "\n",
      "stdout:\n",
      "\n",
      "\n",
      "\n",
      "stderr:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/oem/Documents/github/magnimind_projects/magpenv/lib/python3.12/site-packages/numba_cuda/numba/cuda/cudadrv/driver.py\", line 254, in ensure_initialized\n",
      "    self.cuInit(0)\n",
      "  File \"/home/oem/Documents/github/magnimind_projects/magpenv/lib/python3.12/site-packages/numba_cuda/numba/cuda/cudadrv/driver.py\", line 304, in safe_cuda_api_call\n",
      "    self._check_ctypes_error(fname, retcode)\n",
      "  File \"/home/oem/Documents/github/magnimind_projects/magpenv/lib/python3.12/site-packages/numba_cuda/numba/cuda/cudadrv/driver.py\", line 372, in _check_ctypes_error\n",
      "    raise CudaAPIError(retcode, msg)\n",
      "numba.cuda.cudadrv.driver.CudaAPIError: [999] Call to cuInit results in CUDA_ERROR_UNKNOWN\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 4, in <module>\n",
      "  File \"/home/oem/Documents/github/magnimind_projects/magpenv/lib/python3.12/site-packages/numba_cuda/numba/cuda/cudadrv/driver.py\", line 269, in __getattr__\n",
      "    self.ensure_initialized()\n",
      "  File \"/home/oem/Documents/github/magnimind_projects/magpenv/lib/python3.12/site-packages/numba_cuda/numba/cuda/cudadrv/driver.py\", line 258, in ensure_initialized\n",
      "    raise CudaSupportError(f\"Error at driver init: {description}\")\n",
      "numba.cuda.cudadrv.error.CudaSupportError: Error at driver init: Call to cuInit results in CUDA_ERROR_UNKNOWN (999)\n",
      "\n",
      "\n",
      "Not patching Numba\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "ename": "CUDARuntimeError",
     "evalue": "cudaErrorUnknown: unknown error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCUDARuntimeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcudf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m equity_df_raw \u001b[38;5;241m=\u001b[39m cf\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/equity_value_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m features_df_raw \u001b[38;5;241m=\u001b[39m cf\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/features_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/github/magnimind_projects/magpenv/lib/python3.12/site-packages/cudf/__init__.py:20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcudf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgpu_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_setup\n\u001b[1;32m     19\u001b[0m _setup_numba()\n\u001b[0;32m---> 20\u001b[0m \u001b[43mvalidate_setup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcupy\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config \u001b[38;5;28;01mas\u001b[39;00m numba_config, cuda\n",
      "File \u001b[0;32m~/Documents/github/magnimind_projects/magpenv/lib/python3.12/site-packages/cudf/utils/gpu_utils.py:55\u001b[0m, in \u001b[0;36mvalidate_setup\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CUDARuntimeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;129;01min\u001b[39;00m notify_caller_errors:\n\u001b[0;32m---> 55\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# If there is no GPU detected, set `gpus_count` to -1\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     gpus_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/github/magnimind_projects/magpenv/lib/python3.12/site-packages/cudf/utils/gpu_utils.py:52\u001b[0m, in \u001b[0;36mvalidate_setup\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m notify_caller_errors \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     32\u001b[0m     cudaError_t\u001b[38;5;241m.\u001b[39mcudaErrorInitializationError,\n\u001b[1;32m     33\u001b[0m     cudaError_t\u001b[38;5;241m.\u001b[39mcudaErrorInsufficientDriver,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m     cudaError_t\u001b[38;5;241m.\u001b[39mcudaErrorApiFailureBase,\n\u001b[1;32m     49\u001b[0m }\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m     gpus_count \u001b[38;5;241m=\u001b[39m \u001b[43mgetDeviceCount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CUDARuntimeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;129;01min\u001b[39;00m notify_caller_errors:\n",
      "File \u001b[0;32m~/Documents/github/magnimind_projects/magpenv/lib/python3.12/site-packages/rmm/_cuda/gpu.py:102\u001b[0m, in \u001b[0;36mgetDeviceCount\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m status, count \u001b[38;5;241m=\u001b[39m cudart\u001b[38;5;241m.\u001b[39mcudaGetDeviceCount()\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m cudart\u001b[38;5;241m.\u001b[39mcudaError_t\u001b[38;5;241m.\u001b[39mcudaSuccess:\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CUDARuntimeError(status)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m count\n",
      "\u001b[0;31mCUDARuntimeError\u001b[0m: cudaErrorUnknown: unknown error"
     ]
    }
   ],
   "source": [
    "import cudf as cf\n",
    "equity_df_raw = cf.read_csv('./data/equity_value_data.csv')\n",
    "features_df_raw = cf.read_csv('./data/features_data.csv')\n",
    "\n",
    "equity_df = equity_df_raw.copy()\n",
    "features_df = features_df_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5d0bf5-595b-4bd2-8492-a42f0005f0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "equity_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b93a2e-9529-4e93-a6de-50a747649a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7a2fa4-dcd6-432b-954d-2e6c42ef4d61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e544ae-b183-4f9e-adfe-ebab41876a79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857d9399-b5a2-4db5-89a4-bf9f5f026531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0433abdc-8453-42e0-ab0f-1ec3b1c802a6",
   "metadata": {},
   "source": [
    "#### a). What percentage of users have churned in the data?\n",
    "A user is *churned* when their equity falls below 10 usd for 28 consecutive calendar days or longer having perviously been at least 10 usd\n",
    "\n",
    "**NOTE** Since no equities falls under 10 usd, threshold is set to a variable usd instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e65c4e-4ebf-49bd-b051-4f709ed91887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set threshold\n",
    "thresh = 12\n",
    "\n",
    "# Step 1: Ensure the timestamp column is datetime and sort the data\n",
    "equity_df['timestamp'] = cf.to_datetime(equity_df['timestamp'])\n",
    "equity_df = equity_df.sort_values(['user_id', 'timestamp'])\n",
    "\n",
    "# Step 2: Flag close_equity below threshold\n",
    "equity_df[f'below_{thresh}'] = (equity_df['close_equity'] < thresh).astype(int)\n",
    "\n",
    "# Step 3: Compute rolling 28-day windows for each user\n",
    "equity_df[f'below_{thresh}_28d'] = equity_df.groupby('user_id')[f'below_{thresh}'].rolling(window=28, min_periods=28).sum().reset_index(0, drop=True)\n",
    "\n",
    "# Step 4: Identify churn (continuous 28 days below $11)\n",
    "equity_df['churn'] = (equity_df[f'below_{thresh}_28d'] == 28).astype(int).copy()\n",
    "\n",
    "# Step 5: Check if the user ever had close_equity >= 11\n",
    "# Group by 'user_id' to find the max close_equity\n",
    "user_max_equity = equity_df.groupby('user_id')['close_equity'].max().reset_index()\n",
    "user_max_equity.rename(columns={\"close_equity\": \"max_equity\"}, inplace=True)\n",
    "\n",
    "# Merge back to associate max_equity with each user_id in the main DataFrame\n",
    "equity_df = equity_df.merge(user_max_equity, on=\"user_id\", how=\"left\")\n",
    "\n",
    "# Add a flag for users who had close_equity >= 11 at some point\n",
    "equity_df[f'above_{thresh}_before'] = (equity_df['max_equity'] >= thresh).astype(int)\n",
    "\n",
    "# Step 6: Filter churned users\n",
    "churned_users = equity_df.loc[\n",
    "    (equity_df['churn'] == 1) & (equity_df[f'above_{thresh}_before'] == 1),\n",
    "    'user_id'\n",
    "].unique()\n",
    "\n",
    "# Step 7: Calculate the churn percentage\n",
    "total_users = equity_df['user_id'].nunique()\n",
    "churn_percentage = (len(churned_users) / total_users) * 100\n",
    "\n",
    "print(f\"Churned Percentage: {churn_percentage:.2f}%\\n\")\n",
    "print(f\"Churned Users: {churned_users}\\n\")\n",
    "print(f\"Total Users: {total_users}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d55fca-a987-4a77-b890-877903a4334e",
   "metadata": {},
   "source": [
    "b). Build a classifier given a user with their features assigns a churn probability for every user and predicts which users will churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd97237-dcab-4c73-95aa-a5c413508701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'churned' column\n",
    "features_df['churned'] = features_df['user_id'].isin(churned_users).astype('int32')\n",
    "features_df['churned'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1add47-ea97-4dad-8e70-4874e4a54f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcdadf8-f7ae-41b7-9518-4dd0397c29d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f11ab2-bca4-4881-ad94-35af73d48195",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df['risk_tolerance'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31903243-60b0-4275-a747-06d9e8aeb812",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df['investment_experience'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beb3870-2b27-42ea-a5db-80308d321b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change features of investment experience to 3\n",
    "features_df['investment_experience'] = features_df['investment_experience'].replace({\n",
    "    'extensive_investment_exp': 'good_investment_exp'\n",
    "})\n",
    "features_df['investment_experience'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78a6b1c-2eec-436f-869c-66003cb49f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df['liquidity_needs'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe150636-a945-4013-b841-695e64088581",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df['platform'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3dd258-9aee-40b0-b6f9-a4ebacf81382",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_df['instrument_type_first_traded'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0157c5c1-783c-409f-8b63-712af461639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use vectorized operations to assign 'non_stock' to all values not equal to 'stock'\n",
    "features_df['instrument_type_first_traded'] = (\n",
    "    features_df['instrument_type_first_traded']\n",
    "    .where(features_df['instrument_type_first_traded'] == 'stock', 'non_stock')\n",
    ")\n",
    "\n",
    "# Check the updated value counts\n",
    "print(features_df['instrument_type_first_traded'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05cbd52-fbe7-4b51-b903-1c1da7e43523",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df['time_horizon'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c65ada-9125-42dd-a23e-4d2e60cb5125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "features_df = features_df.to_pandas()\n",
    "\n",
    "# Drop `user_id` and `churned` columns for training\n",
    "features_df = features_df.drop(columns=['user_id'])\n",
    "\n",
    "# Identify categorical and numerical features\n",
    "categorical_features = ['risk_tolerance', 'investment_experience', 'liquidity_needs', 'platform', \n",
    "                        'instrument_type_first_traded', 'time_horizon']\n",
    "numerical_features = ['time_spent', 'first_deposit_amount']\n",
    "\n",
    "# Define preprocessing for numerical and categorical features\n",
    "numerical_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(sparse_output=False, handle_unknown='ignore', min_frequency=0.01)\n",
    "\n",
    "# Combine preprocessors in a column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define anomaly detection models and parameter grids\n",
    "param_grids = {\n",
    "    'IsolationForest': {\n",
    "        'model__contamination': [0.005, 0.01, 0.02, 0.05],\n",
    "        'model__max_samples': [128, 256, 'auto'],\n",
    "        'model__max_features': [0.25, 0.5, 0.75, 1.0]\n",
    "    },\n",
    "    'LocalOutlierFactor': {\n",
    "        'model__n_neighbors': [10, 20, 35, 50],\n",
    "        'model__leaf_size': [10, 25, 30, 50],\n",
    "        'model__contamination': [0.005, 0.01, 0.02, 0.05]\n",
    "    },\n",
    "    'OneClassSVM': {\n",
    "        'model__nu': [0.005, 0.01, 0.05, 0.1],\n",
    "        'model__gamma': ['scale', 'auto'],\n",
    "        'model__kernel': ['rbf', 'poly', 'linear']\n",
    "    }\n",
    "}\n",
    "\n",
    "true_labels = features_df['churned']  # Ground truth labels for evaluation\n",
    "\n",
    "for name, param_grid in param_grids.items():\n",
    "    if name == 'IsolationForest':\n",
    "        model = IsolationForest(random_state=42)\n",
    "    elif name == 'LocalOutlierFactor':\n",
    "        model = LocalOutlierFactor(novelty=True)\n",
    "    elif name == 'OneClassSVM':\n",
    "        model = OneClassSVM()\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid,\n",
    "        scoring='f1_macro',  # Adjust scoring based on your metric of interest\n",
    "        cv=3\n",
    "    )\n",
    "\n",
    "    grid_search.fit(features_df.drop(columns=['churned']), true_labels)\n",
    "    best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "    # Predict using the best pipeline\n",
    "    predictions = best_pipeline.predict(features_df.drop(columns=['churned']))\n",
    "    binary_predictions = (predictions == 1).astype(int)\n",
    "\n",
    "    # Print classification report\n",
    "    print(f\"\\n{name} Best Parameters: {grid_search.best_params_}\\n\")\n",
    "    print(f\"{name} Classification Report:\\n\")\n",
    "    print(classification_report(true_labels, binary_predictions))\n",
    "\n",
    "    # Print feature importances if available\n",
    "    if hasattr(best_pipeline.named_steps['model'], 'feature_importances_'):\n",
    "        feature_importances = best_pipeline.named_steps['model'].feature_importances_\n",
    "        feature_names = numerical_features + list(best_pipeline.named_steps['preprocessor']\n",
    "                                                  .transformers_[1][1]\n",
    "                                                  .get_feature_names_out(categorical_features))\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': feature_importances\n",
    "        }).sort_values(by='Importance', ascending=False)\n",
    "        print(f\"\\n{name} Feature Importances:\\n\")\n",
    "        print(importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e0ffab-936d-4c0e-8e81-4123296cca5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (magpenv)",
   "language": "python",
   "name": "magpenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
